{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Backend Modules for Fraud Detection (Colab Version)\n",
    "\n",
    "This notebook contains the updated code for the backend modules that load the dataset, preprocess data, train models, and run inference based on a dataset with the following columns:\n",
    "\n",
    "- click_id\n",
    "- timestamp\n",
    "- user_id\n",
    "- ip_address\n",
    "- device_type\n",
    "- browser\n",
    "- operating_system\n",
    "- referrer_url\n",
    "- page_url\n",
    "- click_duration\n",
    "- scroll_depth\n",
    "- mouse_movement\n",
    "- keystrokes_detected\n",
    "- ad_position\n",
    "- click_frequency\n",
    "- time_since_last_click\n",
    "- device_ip_reputation\n",
    "- VPN_usage\n",
    "- proxy_usage\n",
    "- bot_likelihood_score\n",
    "- is_fraudulent\n",
    "\n",
    "All references to the file directory now use the current working directory (`os.getcwd()`), which is suitable for Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_raw_data(file_path='data/click_dataset.csv'):\n",
    "    # In Colab, use os.getcwd() since __file__ is not defined\n",
    "    base_dir = os.getcwd()\n",
    "    full_path = os.path.join(base_dir, file_path)\n",
    "    df = pd.read_csv(full_path)\n",
    "    \n",
    "    # Parse timestamp column using the explicit format\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "    \n",
    "    # Remove any rows with invalid timestamps\n",
    "    df = df[df['timestamp'].notna()]\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # Derive temporal features\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Convert device_ip_reputation: map \"Good\" to 0 and \"Suspicious\" to 1\n",
    "    if 'device_ip_reputation' in df.columns:\n",
    "        df['device_ip_reputation'] = df['device_ip_reputation'].map({'Good': 0, 'Suspicious': 1})\n",
    "    \n",
    "    # Ensure VPN_usage and proxy_usage are numeric\n",
    "    df['VPN_usage'] = pd.to_numeric(df['VPN_usage'], errors='coerce')\n",
    "    df['proxy_usage'] = pd.to_numeric(df['proxy_usage'], errors='coerce')\n",
    "    \n",
    "    # For numeric features, convert to numbers and fill missing values with the median\n",
    "    numeric_cols = ['click_duration', 'scroll_depth', 'mouse_movement', 'keystrokes_detected', \n",
    "                    'click_frequency', 'time_since_last_click', 'bot_likelihood_score']\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_data():\n",
    "    df = load_raw_data()\n",
    "    df = feature_engineering(df)\n",
    "    \n",
    "    # Drop columns that are not used for training\n",
    "    drop_cols = ['click_id', 'user_id', 'ip_address', 'timestamp', 'referrer_url', 'page_url']\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns])\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=['is_fraudulent'])\n",
    "    y = df['is_fraudulent'].astype(int)\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    categorical_features = ['device_type', 'browser', 'operating_system', 'ad_position']\n",
    "    X = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X, y = preprocess_data()\n",
    "    print('Processed data shape:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Using the preprocess_data function from the previous cell\n",
    "def load_data():\n",
    "    X, y = preprocess_data()\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_type=''):\n",
    "    if model_type == 'Neural Network':\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype('int32')\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_pred_proba[:, 1] > threshold).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    if model_type != 'Neural Network':\n",
    "        auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "        print(f'{model_type} Accuracy: {accuracy:.4f}, ROC AUC: {auc:.4f}')\n",
    "    else:\n",
    "        print(f'{model_type} Accuracy: {accuracy:.4f}')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return accuracy, report\n",
    "\n",
    "def train_random_forest(X_train, y_train):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "    rf = RandomForestClassifier(n_estimators=300, max_depth=20, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_res, y_res)\n",
    "    return rf\n",
    "\n",
    "def train_xgboost(X_train, y_train):\n",
    "    undersampler = RandomUnderSampler(random_state=42)\n",
    "    X_res, y_res = undersampler.fit_resample(X_train, y_train)\n",
    "    xgb_model = xgb.XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=6,\n",
    "                                  random_state=42, use_label_encoder=False, eval_metric='auc')\n",
    "    xgb_model.fit(X_res, y_res)\n",
    "    return xgb_model\n",
    "\n",
    "def train_neural_network(X_train, y_train):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X_res, y_res, epochs=30, batch_size=64, validation_split=0.2, verbose=1)\n",
    "    return model\n",
    "\n",
    "def save_model(model, filename):\n",
    "    # In Colab, use os.getcwd() for the base directory\n",
    "    base_dir = os.getcwd()\n",
    "    full_filename = os.path.join(base_dir, filename)\n",
    "    dir_name = os.path.dirname(full_filename)\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    if hasattr(model, 'save'):\n",
    "        model.save(full_filename)\n",
    "    else:\n",
    "        with open(full_filename, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    print(f'Model saved to {full_filename}')\n",
    "\n",
    "def train_and_save_models():\n",
    "    X_train, X_test, y_train, y_test = load_data()\n",
    "    base_dir = os.getcwd()\n",
    "    models_dir = os.path.join(base_dir, 'models')\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "    \n",
    "    rf_model = train_random_forest(X_train, y_train)\n",
    "    rf_acc, rf_report = evaluate_model(rf_model, X_test, y_test, 'Random Forest')\n",
    "    save_model(rf_model, os.path.join('models', 'random_forest.pkl'))\n",
    "    \n",
    "    xgb_model = train_xgboost(X_train, y_train)\n",
    "    xgb_acc, xgb_report = evaluate_model(xgb_model, X_test, y_test, 'XGBoost')\n",
    "    save_model(xgb_model, os.path.join('models', 'xgboost.pkl'))\n",
    "    \n",
    "    nn_model = train_neural_network(X_train, y_train)\n",
    "    nn_acc, nn_report = evaluate_model(nn_model, X_test, y_test, 'Neural Network')\n",
    "    save_model(nn_model, os.path.join('models', 'neural_network.h5'))\n",
    "    \n",
    "    scores = {\n",
    "        'random_forest': {'accuracy': rf_acc, 'report': rf_report},\n",
    "        'xgboost': {'accuracy': xgb_acc, 'report': xgb_report},\n",
    "        'neural_network': {'accuracy': nn_acc, 'report': nn_report}\n",
    "    }\n",
    "    with open(os.path.join(models_dir, 'model_scores.json'), 'w') as f:\n",
    "        json.dump(scores, f)\n",
    "    print('Model scores saved.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_and_save_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "def load_models():\n",
    "    # In Colab, use os.getcwd() to set the base directory\n",
    "    base_dir = os.getcwd()\n",
    "    models_dir = os.path.join(base_dir, 'models')\n",
    "    models = {\n",
    "        'random_forest': pickle.load(open(os.path.join(models_dir, 'random_forest.pkl'), 'rb')),\n",
    "        'xgboost': pickle.load(open(os.path.join(models_dir, 'xgboost.pkl'), 'rb')),\n",
    "        'neural_network': tf.keras.models.load_model(os.path.join(models_dir, 'neural_network.h5'))\n",
    "    }\n",
    "    return models\n",
    "\n",
    "def preprocess_input(data):\n",
    "    df = pd.DataFrame([data])\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "        df['hour'] = df['timestamp'].dt.hour\n",
    "        df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "        df.drop(columns=['timestamp'], inplace=True)\n",
    "    if 'device_ip_reputation' in df.columns:\n",
    "        df['device_ip_reputation'] = df['device_ip_reputation'].map({'Good': 0, 'Suspicious': 1})\n",
    "    categorical_features = ['device_type', 'browser', 'operating_system', 'ad_position']\n",
    "    df = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "    return df\n",
    "\n",
    "def predict(input_data):\n",
    "    models = load_models()\n",
    "    processed_data = preprocess_input(input_data)\n",
    "    predictions = {}\n",
    "    for key, model in models.items():\n",
    "        try:\n",
    "            if key == 'neural_network':\n",
    "                pred = (model.predict(processed_data) > 0.5).astype('int32')[0][0]\n",
    "            else:\n",
    "                pred = model.predict(processed_data)[0]\n",
    "            predictions[key] = int(pred)\n",
    "        except Exception as e:\n",
    "            predictions[key] = None\n",
    "    return predictions\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sample_data = {\n",
    "        'click_id': 'sample',\n",
    "        'timestamp': '2024-08-23 02:47:39',\n",
    "        'user_id': 'sample_user',\n",
    "        'ip_address': '141.36.49.37',\n",
    "        'device_type': 'Tablet',\n",
    "        'browser': 'Safari',\n",
    "        'operating_system': 'Android',\n",
    "        'referrer_url': 'https://evans-ford.com/',\n",
    "        'page_url': 'http://www.turner-stewart.com/',\n",
    "        'click_duration': 0.29,\n",
    "        'scroll_depth': 60,\n",
    "        'mouse_movement': 111,\n",
    "        'keystrokes_detected': 8,\n",
    "        'ad_position': 'Bottom',\n",
    "        'click_frequency': 7,\n",
    "        'time_since_last_click': 72,\n",
    "        'device_ip_reputation': 'Good',\n",
    "        'VPN_usage': 0,\n",
    "        'proxy_usage': 1,\n",
    "        'bot_likelihood_score': 0.29\n",
    "    }\n",
    "    result = predict(sample_data)\n",
    "    print('Inference results:', result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
